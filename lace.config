#!crnn/rnn.py
# kate: syntax python;
# see also file:///u/zeyer/setups/quaero-en/training/quaero-train11/50h/ann/2015-07-29--lstm-gt50/config-train/dropout01.3l.n500.custom_lstm.adam.lr1e_3.config

import os
from subprocess import check_output

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

_cf_cache = {}

def cf(filename):
	"""Cache manager"""
	if filename in _cf_cache:
		return _cf_cache[filename]
	if check_output(["hostname"]).strip().decode("utf8") in ["cluster-cn-211", "sulfid"]:
		print("use local file: %s" % filename)
		return filename  # for debugging
	cached_fn = check_output(["cf", filename]).strip().decode("utf8")
	assert os.path.exists(cached_fn)
	_cf_cache[filename] = cached_fn
	return cached_fn

# data
num_inputs = 40  # Gammatone 40-dim
num_outputs = 9001 # 4501
EpochSplit = 6

def get_sprint_dataset(data):
	assert data in ["train", "cv"]
	epochSplit = {"train": EpochSplit, "cv": 1}

	# see /u/tuske/work/ASR/switchboard/corpus/readme
	# and zoltans mail https://mail.google.com/mail/u/0/#inbox/152891802cbb2b40
	files = {}
	files["config"] = "config/training.config"
	files["corpus"] = "/u/tuske/work/ASR/switchboard/corpus/xml/train.corpus.gz"
	files["segments"] = "dependencies/seg_%s" % {"train":"train", "cv":"cv_head3000"}[data]
	files["features"] = "/u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.bundle"
	files["lexicon"] = "/u/tuske/work/ASR/switchboard/corpus/train.lex.v1_0_3.ci.gz"
	files["alignment"] = "dependencies/tuske__2016_01_28__align.combined.train"
	files["cart"] = "/u/tuske/work/ASR/switchboard/initalign/data/%s" % {9001: "cart-9000"}[num_outputs]
	for k, v in sorted(files.items()):
		assert os.path.exists(v), "%s %r does not exist" % (k, v)
	estimated_num_seqs = {"train": 227047, "cv": 3000}  # wc -l segment-file

	# features: /u/tuske/work/ASR/switchboard/feature.extraction/gt40_40/data/gt.train.*
	args = [
	"--config=" + files["config"],
	lambda: "--*.corpus.file=" + cf(files["corpus"]),
	lambda: "--*.corpus.segments.file=" + cf(files["segments"]),
	{"train": "--*.corpus.segment-order-shuffle=true", "cv": "--*.corpus.segment-order-sort-by-time-length=true"}[data],
	"--*.state-tying.type=cart",
	lambda: "--*.state-tying.file=" + cf(files["cart"]),
	"--*.trainer-output-dimension=%i" % num_outputs,
	lambda: "--*.lexicon.file=" + cf(files["lexicon"]),
	lambda: "--*.alignment-cache-path=" + cf(files["alignment"]),
	lambda: "--*.feature-cache-path=" + cf(files["features"]),
	#"--*.mean-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/mean",
	#"--*.variance-normalization.file=dependencies/setup-base/step254-hybrid-mlp-ibm-cmllr/mlp.4/train_mlp/normalize_layer1/std",
	"--*.log-channel.file=log/crnn.sprint.train-dataset.xml",
	"--*.window-size=1",
	"--*.trainer-output-dimension=%i" % num_outputs
	]
	return {
	"class": "ExternSprintDataset", "sprintTrainerExecPath": "sprint-executables/nn-trainer",
	"sprintConfigStr": args,
	"partitionEpoch": epochSplit[data],
	"estimated_num_seqs": estimated_num_seqs[data] // (epochSplit[data] or 1)}
train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
network = {}

dropout = 0
l2 = 0
batch_norm = False
with_bias = not batch_norm
filter_size = (3, 3)

def add_jump_net(i, j, dilation, channel_num):
    source = "jnet%i_%i" % (i, j - 1)
    #here I need to keep the channel, height and width 
    network["jnet%i_%i_conv1" % (i, j)] = { "class": "conv", "n_out": channel_num, "filter_size": filter_size, "dilation_rate": dilation, "padding": "same", "activation": None, "with_bias": with_bias, "dropout": dropout, "L2": l2, "batch_norm": batch_norm, "from": [source] }
    network["jnet%i_%i_relu1" % (i, j)] = {"class": "activation", "activation": "relu", "from": ["jnet%i_%i_conv1" % (i, j)]}
    
    #here I also need to keep the channel, height and width
    network["jnet%i_%i_conv2" % (i, j)] = { "class": "conv", "n_out": channel_num, "filter_size": filter_size, "dilation_rate": dilation, "padding": "same", "activation": None, "with_bias": with_bias, "dropout": dropout, "L2": l2, "batch_norm": batch_norm, "from": ["jnet%i_%i_relu1" % (i, j)] }
    
    network["jnet%i_%i_add" % (i, j)] = {"class": "combine", "kind": "add", "from": [source, "jnet%i_%i_conv2" % (i, j)]}
    network["jnet%i_%i_batch" % (i, j)] = {"class" : "batch_norm", "from": ["jnet%i_%i_add" % (i, j)]}
    network["jnet%i_%i_relu2" % (i, j)] = {"class": "activation", "activation": "relu", "from": ["jnet%i_%i_batch" % (i, j)]}
    network["jnet%i_%i" % (i, j)] = {"class": "copy", "from": ["jnet%i_%i_relu2" % (i, j)]}
    

def add_jump_block(i, num, dr, channel_num):
    source = "jblock%i" % (i - 1)
    
    #here I need to increase the channel, but reduce height and width
    
    network["jnet%i_0" % i] = { "class": "conv", "n_out": channel_num, "filter_size": filter_size, "dilation_rate": dr, "padding": "valid", "activation": None, "with_bias": with_bias, "dropout": dropout, "L2": l2, "batch_norm": batch_norm, "from": [source] }

    for j in range(num):
        add_jump_net(i, j + 1, dr, channel_num)
        
    #element-wise matrix product?
    network["jblock%i" % i] = {"class": "??", "from": ["jnet%i_%i" % (i, num + 1)]}
    
def build_lace(M,N, dilation):
    network["jblock0"] = { "class": "copy", "from": ["data"]}
    channel_num = 2
    for i in range(M):
        channel_num = channel_num * 2
        add_jump_block(i+1, N, dilation, channel_num)

    #here weighted sum across width and height 
    network["conv"] = { "class": "conv", "n_out": 1, "filter_size": filter_size, "dilation_rate": dilation, "padding": "same", "activation": None, "with_bias": with_bias, "dropout": dropout, "L2": l2, "batch_norm": batch_norm, "from": ["jblock%i" % (M + 1)] }
    network["output"] = { "class" : "softmax", "loss" : "ce", "from" : ["conv"] }


build_lace(5, 10 ,1)

# trainer
batching = "random"
batch_size = 5000
max_seqs = 40
chunking = "200"  # "50:25"
truncation = -1
num_epochs = 10
#pretrain = "default"
#pretrain_construction_algo = "from_output"
gradient_clip = 0
adam = True
gradient_noise = 0.3
learning_rate = 0.0005
learning_rate_file = "newbob.data"
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_relative_error_relative_lr = True
newbob_multi_num_epochs = EpochSplit
newbob_multi_update_interval = 1
model = "net-model/network"

# log
log = "log/crnn.train.log"
log_verbosity = 5

